{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Statistic theory goes the opposite way from probability theory. In reality what we observe is a sample of data, but we don't know from what mechanism the data are generated. Statistics asks how can we infer the data generating process from the sample that we observe. For example, if we assume the data are generated from a normal distribution, what is the best way that we learn the population variance? Does the data provide evidence that two samples are drawn from population of the same means? How sure we are about tomorrow's stock price?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Example** The sample of 100 accounts from\n",
    "[HK top 300 Youtubers](https://www.kaggle.com/datasets/patriotboy112/hks-top-300-youtubers) in the previous lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "load(\"logYoutuber_sample.Rdata\")\n",
    "n <- nrow(d1.log.sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Point estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mu <- mean(d1.log.sub[[\"count\"]])\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "sigma <- sd(d1.log.sub[[\"count\"]])\n",
    "print(sigma^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We are interested in the *sample mean* to learn the location of the center in the probability weighted average, or *population mean* $\\mu = E[X_i]$.\n",
    "We are also interested in the *sample variance* to learn the scale of spread of the sample, or *population variance* $\\sigma^2 = var(X_i)$.\n",
    "\n",
    "View the sample as a multinomial distribution with $n$ outcomes at the observed realized points. \n",
    "This is a sample, so each observation is equally important.\n",
    "Assign equal probability $1/n$ on each point, and naturally the sample mean is $\\hat{\\mu} = \\bar{X} = n^{-1} \\sum_{i=1}^n X_i.$\n",
    "Similarly, a widely used sample variance is computed as $s^2 = (n-1)^{-1} \\sum_{i=1}^n (X_i - \\hat{\\mu})^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, for $p>0$ the statistic $n^{-1} \\sum_{i=1}^n x_i^p$ is called the $p$-th sample moment, and $n^{-1} \\sum_{i=1}^n (X_i - \\bar{X})^p$ is called the $p$-th centered moment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Estimation methods\n",
    "\n",
    "The familiar mean and variance are two examples. \n",
    "Suppose the data are generated from a parametric model, Statistical\n",
    "estimation looks for the unknown parameter from the observed data. A\n",
    "*principle* is an ideology about a proper way of estimation. Over the\n",
    "history of statistics, only a few principles are widely accepted.\n",
    "There are multiple ways to estimate parameters. \n",
    "Two most popular methods are the *method of moments* and the *maximum likelihood method*. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Method of moments\n",
    "\n",
    "We express some moments as functions \n",
    "of these parameters. \n",
    "We use the sample moments to mimic the corresponding population moments. \n",
    "Then we invert the functions to back out the parameters. This is called the method of moments.\n",
    "\n",
    "**Example**, a normal distribution is completely characterized by $\\mu$ and $\\sigma^2$. The above two estimates \n",
    "are the method of moments estimates of the two parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x <- rnorm(20)\n",
    "mean(x)\n",
    "var(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "**Example**: \n",
    "\n",
    "$t$ distribution has one parameter, its degree of freedom $\\nu$. The variance of $t(\\nu)$, if $\\nu >2$,  is  $\\sigma^2 = \\nu / (\\nu - 2)$.\n",
    "From data, we estimate $\\hat{\\sigma}^2$, and then solve $\\hat{\\sigma}^2 = \\nu / (\\nu - 2)$ to get\n",
    "\n",
    "$$\n",
    "  \\hat{\\nu} = \\frac{2\\hat{\\sigma}^2}{\\hat{\\sigma}^2 - 1}\n",
    "$$\n",
    "\n",
    "Note: The population variance of $t$ distribution is no smaller than 1.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Maximum likelihood\n",
    "\n",
    "Looking for the value of the parameter that maximizes the value of the log-likelihood. Why is this a good strategy? Because by the concavity of logarithm, in population the true parameter is the value that maximize the population maximum likelihood. In reality, we maximize its empirical version. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Consider a random sample of\n",
    "drawn from a parametric\n",
    "distribution with density $f_{x}\\left(x_{i};\\theta\\right)$, where\n",
    "$z_{i}$ is either a scalar random variable or a random vector. A\n",
    "parametric distribution is completely characterized by a\n",
    "finite-dimensional parameter $\\theta$. We know that $\\theta$ belongs to\n",
    "a parameter space $\\Theta$. We use the data to estimate $\\theta$.\n",
    "\n",
    "The log-likelihood of observing the entire sample $X=(X_1,X_2,\\ldots,X_n)$ is\n",
    "\n",
    "$$\n",
    "  L_{n}\\left(\\theta;X\\right):=\\log\\left(\\prod_{i=1}^{n}f_{X}\\left(X_{i};\\theta\\right)\\right)=\\sum_{i=1}^{n}\\log f_{X}\\left(X_{i};\\theta\\right).\n",
    "$$\n",
    "\n",
    "In reality the sample $X$ is given and for each $\\theta\\in\\Theta$ we can\n",
    "evaluate $L_{n}\\left(\\theta;X\\right)$. The maximum likelihood estimator\n",
    "is\n",
    "\n",
    "$$\\widehat{\\theta}_{MLE}:=\\arg\\max_{\\theta\\in\\Theta}L_{n}\\left(\\theta;X\\right).$$\n",
    "\n",
    "Why maximizing the log-likelihood function is desirable? An intuitive\n",
    "explanation is that $\\widehat{\\theta}_{MLE}$ makes observing $X$ the\n",
    "“most likely” in the entire parametric space.\n",
    "\n",
    "A formal justification of MLE uses the terminology the *Kullback-Leibler divergence*,\n",
    "which is a measurement of the \"distance\" between two distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Example**:\n",
    "Consider the Gaussian location model $X_{i}\\sim N\\left(\\mu,1\\right)$,\n",
    "where $\\mu$ is the unknown parameter to be estimated. The likelihood of\n",
    "observing $X_{i}$ is\n",
    "$f_{X}\\left(X_{i};\\mu\\right)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(X_{i}-\\mu\\right)^{2}\\right)$.\n",
    "The likelihood of observing the sample $X$ is\n",
    "\n",
    "$$f_{X}\\left(X;\\mu\\right)=\\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(X_{i}-\\mu\\right)^{2}\\right)$$\n",
    "\n",
    "and the log-likelihood is\n",
    "\n",
    "$$L_{n}\\left(\\mu;X\\right)=-\\frac{n}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}\\sum_{i=1}^{n}\\left(X_{i}-\\mu\\right)^{2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the above example, if the variance is unknown, it can also be added as a parameter and participate the optimization. \n",
    "The MLE estimator of the variance, in this example of normal distribution, is $\\hat{\\sigma}^2 = n^{-1} \\sum_{i=1}^n (X_i - \\bar{X})^2$.\n",
    "Notice the denominator here is $n$, instead of the previous $(n-1)$ as in $s^2$. While the fact that the latter adjustment is unbiased suggests the MLE\n",
    "estimator is biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Interval estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "CI <- function(x) {# x is a vector of random variables\n",
    "  # nominal coverage probability is 95%\n",
    "  n <- length(x)\n",
    "  mu <- mean(x)\n",
    "  sig <- sd(x)\n",
    "  upper <- mu + 1.96 / sqrt(n) * sig\n",
    "  lower <- mu - 1.96/ sqrt(n) * sig\n",
    "  return(list(lower = lower, upper = upper))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "CI(d1.log.sub[[\"count\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Statistic Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The above calculation are mechanical. Why do we want to do with them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Relate the statistic with the population. \n",
    "Suppose $X_i$ are random draws from a population distribution $P$. \n",
    "We are interested in the mean $\\mu: = E[X_i]$?\n",
    "The same mean is a natural estimator (guess) of $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Confidence Interval\n",
    "\n",
    "The estimator $\\hat{\\mu}$ has a sampling error. If we draw another sample from the same population, the sample mean is likely to be a different number.\n",
    "We must characterize the uncertainty.\n",
    "We have provided an interval estimator. What does it mean?\n",
    "An interval estimator is only meaningful when we can figure out its *sampling distribution*.\n",
    "The sampling distribution is the distribution of a statistic. \n",
    "It characterizes the uncertainty of the statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Example**: sampling distributions. \n",
    "\n",
    "Under normal distribution\n",
    "\n",
    "* Assume $X_i \\sim N(\\mu, \\sigma^2)$ with a known variance. Then $\\bar{X} \\sim N(\\mu, \\sigma^2 / n)$.\n",
    "* Assume $X_i \\sim N(\\mu, \\sigma^2)$ with a unknown variance. Then $\\frac{\\bar{X} - \\mu}{s} \\sim t(n-1)$.\n",
    "* Assume $X_i \\sim (\\mu, \\sigma^2)$ with a finite variance variance. \n",
    "$\\sqrt{n} \\frac{\\bar{X} - \\mu}{\\sigma} \\stackrel{d}{\\rightarrow} N(0, 1)$. (to be discussed later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Simulation examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "empirical coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "Rep <- 1000\n",
    "sample_size <- 10\n",
    "capture <- rep(0, Rep)\n",
    "Bounds <- matrix(0, nrow = Rep, ncol = 2)\n",
    "for (i in 1:Rep) {\n",
    "  x <- rpois(sample_size, 2)\n",
    "  bounds <- CI(x)\n",
    "  capture[i] <- ((bounds$lower <= mu) & (mu <= bounds$upper))\n",
    "  Bounds[i,] <- unlist( bounds )\n",
    "}\n",
    "cat(\"the emprical coverage probability = \", mean(capture)) # empirical size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "25 replications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "Bounds25 <- Bounds[1:25, ]\n",
    "plot(1, type=\"n\", xlab=\"\", ylab=\"\", \n",
    "     ylim=c(min(Bounds25), max(Bounds25)), xlim=c(1, 25))\n",
    "segments(x0= 1:25, y0=Bounds25[,1], x1 = 1:25, y1 = Bounds25[,2])\n",
    "abline(h=2, col = \"red\", lty = 2, lwd = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Coverage probability* is the probability that the estimated confidence interval coverage the true value in repeated sampling.\n",
    "Here the confidence interval is random while the true value is fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It is important to understand that it is not the probability that the parameter falls into a fixed interval, which is the interpretation of the *Bayesian credit interval*. \n",
    "The latter requires different way for construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc42d5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "A *hypothesis* is a statement about the parameter space $\\Theta$.\n",
    "Hypothesis testing checks whether the data support a *null hypothesis*\n",
    "$\\Theta_{0}$, which is a subset of $\\Theta$ of interest. Ideally the\n",
    "null hypothesis should be suggested by scientific theory. The\n",
    "*alternative hypothesis* $\\Theta_{1}=\\Theta\\backslash\\Theta_{0}$ is the\n",
    "complement of $\\Theta_{0}$. Based on the observed evidence, hypothesis\n",
    "testing decides to accept or reject the null hypothesis. If the null\n",
    "hypothesis is rejected by the data, it implies that from the statistical\n",
    "perspective the data is incompatible with the proposed scientific\n",
    "theory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eaa581",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "------------------------------------------------------------------------\n",
    "                    |   accept $H_{0}$  |   reject $H_{0}$\n",
    "      $H_{0}$ true  |  correct decision |    Type I error\n",
    "      $H_{0}$ false |   Type II error   |  correct decision\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "Let $\\phi(X;\\theta)$ as the *decision function*. It takes value \"1\" when $\\theta$ is rejected as a null hypothesis, and it takes value \"0\" otherwise.\n",
    "We define the power function $\\beta(\\theta) = E[ \\phi(X;\\theta) ]$, which is the probability of rejecting a null hypothesis $\\theta$.\n",
    "\n",
    "Actions, States and Consequences\n",
    "\n",
    "-   The *probability of committing Type I error* is\n",
    "    $\\beta\\left(\\theta\\right)$ for some $\\theta\\in\\Theta_{0}$.\n",
    "\n",
    "-   The *probability of committing Type II error* is\n",
    "    $1-\\beta\\left(\\theta\\right)$ for some $\\theta\\in\\Theta_{1}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa5c7d4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "The philosophy on hypothesis testing has been debated for centuries. At\n",
    "present the prevailing framework in statistics textbooks is the\n",
    "*frequentist perspective*. A frequentist views the parameter as a fixed\n",
    "constant. They keep a conservative attitude about the Type I error: Only\n",
    "if overwhelming evidence is demonstrated shall a researcher reject the\n",
    "null. Under the principle of protecting the null hypothesis, a desirable\n",
    "test should have a small level. Conventionally we take $\\alpha=0.01,$\n",
    "0.05 or 0.1. We say a test is *unbiased* if\n",
    "$\\beta\\left(\\theta\\right)>\\sup_{\\theta\\in\\Theta_{0}}\\beta\\left(\\theta\\right)$\n",
    "for all $\\theta\\in\\Theta_{1}$. There can be many tests of correct size.\n",
    "\n",
    "A trivial test function\n",
    "$\\phi({x})=1\\left\\{ 0\\leq U\\leq\\alpha\\right\\}$ for all\n",
    "$\\theta\\in\\Theta$, where $U$ is a random variable from a uniform\n",
    "distribution on $\\left[0,1\\right]$, has correct size $\\alpha$ but no\n",
    "non-trivial power at the alternative. On the other extreme, the trivial\n",
    "test function $\\phi(x)=1$ for all $x$\n",
    "enjoys the biggest power but suffers incorrect size.\n",
    "\n",
    "Usually, we design a test by proposing a test statistic\n",
    "$T_{n}$ and a corresponding critical value\n",
    "$c_{1-\\alpha}$. Given $T_{n}$ and $c_{1-\\alpha}$, we write the test\n",
    "function as\n",
    "\n",
    "$$\n",
    "\\phi(X)=1\\left\\{ T_{n} (X)>c_{1-\\alpha}\\right\\}.\n",
    "$$\n",
    "\n",
    "To ensure such a $\\phi(x)$ has correct size, we need\n",
    "to figure out the distribution of $T_{n}$ under the null hypothesis\n",
    "(called the *null distribution*), and choose a critical value\n",
    "$c_{1-\\alpha}$ according to the null distribution and the desirable size\n",
    "or level $\\alpha$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# set the null hypothesis as 6\n",
    "(mu-6)/(sigma/sqrt(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "Another commonly used indicator in hypothesis testing is $p$-value:\n",
    "\n",
    "$$\\sup_{\\theta\\in\\Theta_{0}}P_{\\theta}\\left\\{ T_{n}\\left({x}\\right)\\leq T_{n}\\left({X}\\right)\\right\\}.$$\n",
    "\n",
    "In the above expression, $T_{n}\\left({x}\\right)$ is the realized\n",
    "value of the test statistic $T_{n}$, while\n",
    "$T_{n}\\left({X}\\right)$ is the random variable generated by\n",
    "${X}$ under the null $\\theta\\in\\Theta_{0}$. The interpretation of\n",
    "the $p$-value is tricky. $p$-value is the probability that we observe\n",
    "$T_{n}({X})$ being greater than the realized $T_{n}({x})$\n",
    "if the null hypothesis is true.\n",
    "\n",
    "$p$-value is *not* the probability that the null hypothesis is true.\n",
    "Under the frequentist perspective, the null hypothesis is either true or\n",
    "false, with certainty. The randomness of a test comes only from\n",
    "sampling, not from the hypothesis. $p$-value measures whether the\n",
    "dataset is compatible with the null hypothesis. $p$-value is closely\n",
    "related to the corresponding test. When $p$-value is smaller than the\n",
    "specified test size $\\alpha$, the test rejects the null.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Connections\n",
    "\n",
    "Hypothesis testing, confidence interval, and $p$-value are tightly connected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Revealing the population\n",
    "\n",
    "The population data of HK top 300 Youtubers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "load(\"logYoutuber.Rdata\")\n",
    "N = nrow(d1.log) # population size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mean(d1.log$count)\n",
    "cor(d1.log$subs,  d1.log$view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "subsample.mean <- function(j) { mean( d1.log$count[sample(1:N, n) ] ) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# average of each \"potential\" sample\n",
    "rep.mean <- plyr::laply(.data = 1:200,\n",
    "                        .fun  = subsample.mean )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "hist(rep.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Multivariate Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For a bivariate random vector $(X_i,Y_i)$,  \n",
    "\n",
    "* Covariance \n",
    "\n",
    "$$\n",
    "\\hat{cov}(X,Y) = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X}) (Y_i - \\bar{Y}) = \\frac{1}{n} \\sum_{i=1}^n X_i Y_i - \\bar{X} \\bar{Y}.\n",
    "$$\n",
    "\n",
    "* Correlation coefficient \n",
    "\n",
    "$$\n",
    "\\hat{\\rho}(X,Y ) = \\frac{\\hat{cov}(X,Y)} { \\sqrt{ \\hat {\\sigma}^2(X) \\hat {\\sigma}^2(Y) }}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "cor(d1.log.sub$subs,  d1.log.sub$view)\n",
    "plot(x = d1.log.sub$subs,  y = d1.log.sub$view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "cor(d1.log$subs,  d1.log$view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Regressions\n",
    "\n",
    "Data fitting. Basic learning task. Get the prediction as close as possible. \n",
    "\n",
    "ANOVA. The decomposition of sample variance. \n",
    "\n",
    "Regression does a very simple task, which in principle has nothing to do with causality. It minimizes the SSR. In other words, it tries to use a linear combination of the $X$ to reduce the variance of Y as much as possible. The any $(X,Y)$, such an algorithm can be implemented. It is a numerical algorithm that has nothing to do with uncertainty.\n",
    "\n",
    "Now we superimpose uncertainty on it. If $X$ is still fixed but Y is uncertain, we have conditional inference (discriminative model. These terms are used in classification, not in regression.) If both $(X,Y)$ are uncertain, we have unconditional inference (generative model). It matters only for interpretation. In reality, both $(X,Y)$ are fixed. 从最庸俗到最抽象. The conditional model is not good for extrapolation out of the support of the observed X. \n",
    "\n",
    "Perhaps use the bivariate regression and elementary algebra to derive the formula. If both $(y,x)$ have unit variance, then the regression coefficient is simply the correlation coefficient. \n",
    "\n",
    "$$\\frac{\\sigma_{xy}}{\\sigma_x^2}=\\frac{\\rho \\sigma_x \\sigma_y}{\\sigma_x^2} = \\rho \\frac{\\sigma_y} {\\sigma_x}$$\n",
    "\n",
    "The matrix version is a more general analogue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The unconditional model can be thought as if it is generated in two steps: \n",
    "\n",
    "First draw $X$, and then draw $Y|X$. We know $P(X,Y) = P(Y|X)P(X)$ so that the discriminative model and the generative model are linked together.) However, in dynamic model such formulation is insufficient, for example the predictive model.\n",
    "\n",
    "Unbiasedness: this is about the uncertainty in beta. At least one component of $X$ and $Y$ must be random. so beta is a random variable and we can discuss its mean and variance. Also the distribution of beta. \n",
    "\n",
    "Why OLS? It is also the MLE under the normal distribution. Normal distribution has a simple form that the mean and variance are sufficient statistic to describe the distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "The beauty of statistics lies in their deep connection with philosophy. A realized sample is a set of given numbers. They are fixed, known, and invariant. But where is uncertainty? Bayesians argue that uncertainty comes from people's belief about the object of interest. The data is a set of information to help us refine or update our belief. In contrast, frequentists contend that uncertainty is present  before the numbers, or realizations, are revealed; once the realizations are revealed, uncertainty is gone. \n",
    "\n",
    "The two schools of philosophical thoughts, Bayesian and frequentist, have been contesting over centuries. Nowadays in academic research and in teaching, frequentist remains the mainstream so it is the one that we are more familiar with. However, frequentist interpretation of probability and the resulting implication for statistical inference are not completely natural. In particular, repeated experiments are more easily perceived in physics and other natural science, but may be unrealistic in many economic settings. On the other hand, the difficulty of Bayesian lies in the consensus about the prior distribution, or the belief. Either has pros and cons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R [conda env:r]",
   "language": "R",
   "name": "conda-env-r-r"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
